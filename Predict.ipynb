{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56a7e8-4e89-4bb0-b5e4-cc15ab52161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download Flickr8k dataset from Kaggle\n",
    "def download_flickr8k():\n",
    "    print(\"Authenticating with Kaggle API...\")\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    dataset_path = 'flickr8k_download'\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "        print(\"Downloading Flickr8k dataset...\")\n",
    "        api.dataset_download_files('adityajn105/flickr8k', path=dataset_path, unzip=True)\n",
    "    else:\n",
    "        print(\"Flickr8k dataset already downloaded.\")\n",
    "    return dataset_path\n",
    "\n",
    "dataset_path = download_flickr8k()\n",
    "images_dir = os.path.join(dataset_path, 'Images')\n",
    "captions_file = os.path.join(dataset_path, 'captions.txt')\n",
    "\n",
    "# Load captions\n",
    "print(\"Loading captions...\")\n",
    "captions = pd.read_csv(captions_file, delimiter=',', header=0, names=['image', 'caption'])\n",
    "print(\"Captions loaded successfully.\")\n",
    "\n",
    "# Load pre-trained model, feature extractor, and tokenizer\n",
    "print(\"Loading pre-trained model, feature extractor, and tokenizer...\")\n",
    "model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Model, feature extractor, and tokenizer loaded.\")\n",
    "\n",
    "# Prepare the dataset\n",
    "def preprocess_function(examples):\n",
    "    images = [Image.open(os.path.join(images_dir, img)).convert(\"RGB\") for img in examples[\"image\"]]\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    labels = tokenizer(examples[\"caption\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    return {\"pixel_values\": [x for x in pixel_values], \"labels\": [x for x in labels]}\n",
    "\n",
    "print(\"Creating dataset from captions...\")\n",
    "dataset = Dataset.from_pandas(captions)\n",
    "print(\"Dataset created. Preprocessing dataset...\")\n",
    "dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"image\", \"caption\"])\n",
    "print(\"Dataset preprocessed.\")\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "print(\"Splitting dataset into train and validation sets...\")\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "val_dataset = train_test_split['test']\n",
    "print(\"Dataset split complete.\")\n",
    "\n",
    "# Define the LoRA configuration based on inspected layers\n",
    "print(\"Setting up LoRA configuration...\")\n",
    "config = LoraConfig(\n",
    "    r=8,             # rank of the LoRA matrix\n",
    "    lora_alpha=32,   # scaling factor for the LoRA update\n",
    "    target_modules=[\n",
    "        \"encoder.encoder.layer.0.attention.attention.query\", \n",
    "        \"encoder.encoder.layer.0.attention.attention.key\", \n",
    "        \"encoder.encoder.layer.0.attention.attention.value\", \n",
    "        \"encoder.encoder.layer.0.attention.output.dense\",\n",
    "        \"decoder.transformer.h.0.crossattention.c_attn\", \n",
    "        \"decoder.transformer.h.0.crossattention.q_attn\", \n",
    "        \"decoder.transformer.h.0.crossattention.c_proj\"\n",
    "    ],  # correct layer names\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "print(\"LoRA configuration set.\")\n",
    "\n",
    "# Create the PEFT model\n",
    "print(\"Creating the PEFT model...\")\n",
    "model = get_peft_model(model, config)\n",
    "model.to(device)\n",
    "print(\"PEFT model created and moved to device.\")\n",
    "\n",
    "# Define training arguments\n",
    "print(\"Setting up training arguments...\")\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,  # Reduced batch size\n",
    "    gradient_accumulation_steps=4,  # Gradient accumulation\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,  # Enable mixed precision training\n",
    ")\n",
    "print(\"Training arguments set.\")\n",
    "\n",
    "# Create a custom data collator\n",
    "class DataCollatorForImageCaptioning:\n",
    "    def __init__(self, feature_extractor, tokenizer):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        pixel_values = torch.stack([torch.tensor(item[\"pixel_values\"]) for item in batch])\n",
    "        labels = torch.stack([torch.tensor(item[\"labels\"]) for item in batch])\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Initialize the Trainer\n",
    "print(\"Initializing the Trainer...\")\n",
    "data_collator = DataCollatorForImageCaptioning(feature_extractor, tokenizer)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"Trainer initialized.\")\n",
    "\n",
    "# Fine-tune the model\n",
    "print(\"Starting fine-tuning...\")\n",
    "torch.cuda.empty_cache()  # Free up unused memory\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete.\")\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "print(\"Saving the fine-tuned model and tokenizer...\")\n",
    "model.save_pretrained(\"./fine-tuned-model\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-model\")\n",
    "print(\"Fine-tuned model and tokenizer saved.\")\n",
    "\n",
    "# Load and use the fine-tuned model for prediction\n",
    "print(\"Loading the fine-tuned model for prediction...\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"./fine-tuned-model\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine-tuned-model\")\n",
    "print(\"Fine-tuned model loaded.\")\n",
    "\n",
    "# Example image URL for prediction\n",
    "url = \"https://images.freeimages.com/images/large-previews/429/plane-1449679.jpg\"\n",
    "print(\"Fetching example image for prediction...\")\n",
    "response = requests.get(url, stream=True)\n",
    "image = Image.open(response.raw).convert(\"RGB\")\n",
    "image.show()\n",
    "\n",
    "# Preprocess the image for prediction\n",
    "print(\"Preprocessing the image for prediction...\")\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate caption\n",
    "print(\"Generating caption...\")\n",
    "with torch.no_grad():\n",
    "    pixel_values = inputs.pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# Print the generated caption\n",
    "print(f\"Generated Caption: {generated_text}\")\n",
    "\n",
    "# Display the image with the caption\n",
    "plt.imshow(image)\n",
    "plt.title(generated_text)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(\"Prediction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d4b9c-60d8-4b60-9d06-fd0230bf6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original and fine-tuned models for comparison\n",
    "print(\"Loading the original and fine-tuned models for comparison...\")\n",
    "original_model = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)\n",
    "fine_tuned_model = VisionEncoderDecoderModel.from_pretrained(\"./fine-tuned-model\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"Models loaded.\")\n",
    "\n",
    "# List of image URLs for prediction\n",
    "image_urls = [\n",
    "    \"https://media.istockphoto.com/id/1500816306/photo/adult-black-male-admiring-the-streets-of-london-on-a-sunny-day-while-holding-a-smartphone-in.webp?s=2048x2048&w=is&k=20&c=0aHww9bA2AdPEUyPkbTiZBFM-ZNeSUY_oZ2nGWAorXI=\",\n",
    "    \"https://images.freeimages.com/images/large-previews/71f/my-new-bicycle-1431529.jpg\",\n",
    "    \"https://images.freeimages.com/images/large-previews/f02/computer-room-1242684.jpg\",\n",
    "    \"https://images.freeimages.com/images/large-previews/4ca/tree-1552037.jpg\",\n",
    "    \"https://images.freeimages.com/images/large-previews/15b/lap-cat-1243719.jpg\",\n",
    "    \"https://images.freeimages.com/images/large-previews/647/snowy-mountain-1378865.jpg\",\n",
    "    \"https://images.freeimages.com/images/large-previews/792/captiol-building-1228390.jpg\",\n",
    "    \"https://images.freeimages.com/images/large-previews/429/plane-1449679.jpg\"\n",
    "]\n",
    "\n",
    "# Iterate through the list of image URLs and make predictions with both models\n",
    "for url in image_urls:\n",
    "    print(f\"Fetching image from URL: {url}\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    image = Image.open(response.raw).convert(\"RGB\")\n",
    "\n",
    "    # Preprocess the image for prediction\n",
    "    print(\"Preprocessing the image for prediction...\")\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate caption with original model\n",
    "    print(\"Generating caption with original model...\")\n",
    "    with torch.no_grad():\n",
    "        pixel_values = inputs.pixel_values\n",
    "        original_generated_ids = original_model.generate(pixel_values)\n",
    "        original_generated_text = tokenizer.batch_decode(original_generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Generate caption with fine-tuned model\n",
    "    print(\"Generating caption with fine-tuned model...\")\n",
    "    with torch.no_grad():\n",
    "        pixel_values = inputs.pixel_values\n",
    "        fine_tuned_generated_ids = fine_tuned_model.generate(pixel_values)\n",
    "        fine_tuned_generated_text = tokenizer.batch_decode(fine_tuned_generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Print the generated captions\n",
    "    print(f\"Original Model Caption: {original_generated_text}\")\n",
    "    print(f\"Fine-tuned Model Caption: {fine_tuned_generated_text}\")\n",
    "\n",
    "    # Display the image with both captions\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Original: {original_generated_text}\\nFine-tuned: {fine_tuned_generated_text}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(\"Prediction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5963ba4c-58f9-421b-be3a-a268b3993b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model layers to identify target modules for LoRA\n",
    "print(\"Inspecting model layers to identify target modules for LoRA...\")\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6fbad-8d6f-40e8-9156-cca22e9782a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
