{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31b737-51cb-425d-ba6a-408dc06b2f9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Get the current working directory\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Define the new directory path relative to the current working directory\n",
    "new_dir = os.path.join(script_dir, 'flickr8k_download')\n",
    "\n",
    "# Create the new directory if it doesn't exist\n",
    "os.makedirs(new_dir, exist_ok=True)\n",
    "\n",
    "# Ensure the kaggle.json file is in the right place\n",
    "kaggle_config_dir = os.path.expanduser('~/.kaggle')\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = kaggle_config_dir\n",
    "\n",
    "# Check if the kaggle.json file exists\n",
    "kaggle_json_path = os.path.join(kaggle_config_dir, 'kaggle.json')\n",
    "if not os.path.exists(kaggle_json_path):\n",
    "    raise FileNotFoundError(f\"{kaggle_json_path} does not exist. Please place the kaggle.json file in the ~/.kaggle directory.\")\n",
    "\n",
    "# Check if the kaggle.json file is readable\n",
    "if not os.access(kaggle_json_path, os.R_OK):\n",
    "    raise PermissionError(f\"{kaggle_json_path} is not readable. Please check the file permissions.\")\n",
    "\n",
    "# Read the kaggle.json file to ensure it's correct\n",
    "try:\n",
    "    with open(kaggle_json_path, 'r') as f:\n",
    "        kaggle_config = json.load(f)\n",
    "    assert 'username' in kaggle_config and 'key' in kaggle_config, \"Invalid kaggle.json file format.\"\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error reading {kaggle_json_path}: {e}\")\n",
    "\n",
    "# Initialize the Kaggle API\n",
    "api = KaggleApi()\n",
    "try:\n",
    "    api.authenticate()\n",
    "    print(\"Successfully authenticated with the Kaggle API.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to authenticate with the Kaggle API: {e}\")\n",
    "\n",
    "# Define the dataset\n",
    "dataset = 'adityajn105/flickr8k'\n",
    "dataset_marker = os.path.join(new_dir, 'dataset_downloaded.marker')\n",
    "\n",
    "# Check if the dataset has already been downloaded\n",
    "if not os.path.exists(dataset_marker):\n",
    "    try:\n",
    "        # Download the dataset to the new directory\n",
    "        api.dataset_download_files(dataset, path=new_dir, unzip=True)\n",
    "        # Create a marker file to indicate the dataset has been downloaded\n",
    "        Path(dataset_marker).touch()\n",
    "        print(f'Dataset downloaded and extracted to {new_dir}')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to download the dataset: {e}\")\n",
    "else:\n",
    "    print(f\"Dataset already downloaded and extracted in {new_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e654f18-a4dd-47f6-9483-7b98cea3e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the number of images to display\n",
    "num_images_to_display = 5  # Change this number to display more or fewer images\n",
    "\n",
    "# Define the path to the downloaded dataset\n",
    "dataset_dir = new_dir\n",
    "images_dir = os.path.join(dataset_dir, 'Images')\n",
    "captions_file = os.path.join(dataset_dir, 'captions.txt')\n",
    "\n",
    "# Check if the images directory and captions file exist\n",
    "if not os.path.exists(images_dir):\n",
    "    raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
    "\n",
    "if not os.path.exists(captions_file):\n",
    "    raise FileNotFoundError(f\"Captions file not found: {captions_file}\")\n",
    "\n",
    "# Read the captions file, skipping the first row if it's a header\n",
    "try:\n",
    "    captions = pd.read_csv(captions_file, delimiter=',', header=0, names=['image', 'caption'])\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error reading captions file: {e}\")\n",
    "\n",
    "# Display the first few rows of the captions\n",
    "print(\"Captions DataFrame:\")\n",
    "print(captions.head())\n",
    "\n",
    "# List and print the first few filenames in the images directory\n",
    "image_files = os.listdir(images_dir)\n",
    "print(\"First few image files in the images directory:\")\n",
    "print(image_files[:5])\n",
    "\n",
    "# Function to display an image with its caption\n",
    "def display_image_with_caption(image_file, caption):\n",
    "    image_path = os.path.join(images_dir, image_file)\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"File not found: {image_path}\")\n",
    "        return\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.title(caption)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display the specified number of images with their captions\n",
    "for idx, row in captions.head(num_images_to_display).iterrows():\n",
    "    display_image_with_caption(row['image'], row['caption'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca17987-bb84-46b8-a557-7a1a00425bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fae7cd-4b7b-490d-b654-28a23840fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load captions and skip the header\n",
    "captions_file = 'flickr8k_download/captions.txt'\n",
    "\n",
    "# Checking the first few lines to ensure correct delimiter\n",
    "with open(captions_file, 'r') as file:\n",
    "    for _ in range(5):\n",
    "        print(file.readline())\n",
    "\n",
    "# Load the captions data\n",
    "captions = pd.read_csv(captions_file, delimiter=',', header=0, names=['image', 'caption'])\n",
    "\n",
    "# Image directory\n",
    "image_dir = 'flickr8k_download/Images'\n",
    "\n",
    "# Preprocess images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    return image\n",
    "\n",
    "# Custom Dataset\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, captions, image_dir, processor):\n",
    "        self.captions = captions\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.captions.iloc[idx, 0])\n",
    "        image = load_image(img_name)\n",
    "        caption = self.captions.iloc[idx, 1]\n",
    "        return image, caption\n",
    "\n",
    "# Initialize the model and processor\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Apply LoRA with updated target modules\n",
    "config = LoraConfig(\n",
    "    r=8,             # rank of the LoRA matrix\n",
    "    lora_alpha=32,   # scaling factor for the LoRA update\n",
    "    target_modules=[\"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.q_proj\", \"self_attn.out_proj\"],  # correct layer names\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.to(device)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = Flickr8kDataset(captions, image_dir, processor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Optimizer and Loss function\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, captions in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        logits_per_text = outputs.logits_per_text\n",
    "\n",
    "        ground_truth = torch.arange(len(images), device=device)\n",
    "\n",
    "        loss = (loss_fn(logits_per_image, ground_truth) + loss_fn(logits_per_text, ground_truth)) / 2\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in dataloader:\n",
    "            images = images.to(device)\n",
    "            inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            logits_per_text = outputs.logits_per_text\n",
    "\n",
    "            ground_truth = torch.arange(len(images), device=device)\n",
    "\n",
    "            loss = (loss_fn(logits_per_image, ground_truth) + loss_fn(logits_per_text, ground_truth)) / 2\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Define the directory where the fine-tuned model will be saved\n",
    "save_directory = 'ftm/fine-tuned-model'\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting epoch {epoch+1}\")\n",
    "    train_loss = train_one_epoch(model, train_dataloader, optimizer, loss_fn, device)\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Save the fine-tuned model and processor\n",
    "model.save_pretrained(save_directory)\n",
    "processor.save_pretrained(save_directory)\n",
    "print(f\"Model and processor saved to {save_directory}\")\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss = evaluate(model, val_dataloader, device)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0bc56-cda1-47aa-a7e4-1aa6c9289f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the directory where the fine-tuned model is saved\n",
    "save_directory = 'ftm/fine-tuned-model'\n",
    "\n",
    "# Load the base model and processor\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "base_model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Apply LoRA with the same configuration used during training\n",
    "config = LoraConfig(\n",
    "    r=8,             # rank of the LoRA matrix\n",
    "    lora_alpha=32,   # scaling factor for the LoRA update\n",
    "    target_modules=[\"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.q_proj\", \"self_attn.out_proj\"],  # correct layer names\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# Wrap the base model with the PeftModel to apply LoRA\n",
    "model = get_peft_model(base_model, config)\n",
    "\n",
    "# Load the fine-tuned weights\n",
    "model = PeftModel.from_pretrained(model, save_directory)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Example image URL\n",
    "# url = \"https://media.istockphoto.com/id/1500816306/photo/adult-black-male-admiring-the-streets-of-london-on-a-sunny-day-while-holding-a-smartphone-in.webp?s=2048x2048&w=is&k=20&c=0aHww9bA2AdPEUyPkbTiZBFM-ZNeSUY_oZ2nGWAorXI=\"\n",
    "# url = \"https://images.freeimages.com/images/large-previews/71f/my-new-bicycle-1431529.jpg\"\n",
    "# url = \"https://images.freeimages.com/images/large-previews/f02/computer-room-1242684.jpg\"\n",
    "# url = \"https://images.freeimages.com/images/large-previews/4ca/tree-1552037.jpg\"\n",
    "# url = \"https://images.freeimages.com/images/large-previews/15b/lap-cat-1243719.jpg\"\n",
    "# url = \"https://images.freeimages.com/images/large-previews/647/snowy-mountain-1378865.jpg\"\n",
    "# url = \"https://images.freeimages.com/images/large-previews/792/captiol-building-1228390.jpg\"\n",
    "url = \"https://images.freeimages.com/images/large-previews/429/plane-1449679.jpg\"\n",
    "response = requests.get(url, stream=True)\n",
    "image = Image.open(response.raw)\n",
    "image.show()\n",
    "\n",
    "# Evaluate with different text prompts\n",
    "texts = [\n",
    "    \"A photo of a cat\",\n",
    "    \"A photo of a dog\", \n",
    "    \"A photo of a car\", \n",
    "    \"A photo of a building\", \n",
    "    \"A photo of a tree\", \n",
    "    \"A photo of a computer\", \n",
    "    \"A photo of a person\", \n",
    "    \"A photo of a landscape\", \n",
    "    \"A photo of a beach\", \n",
    "    \"A photo of a mountain\", \n",
    "    \"A photo of a city\", \n",
    "    \"A photo of a forest\", \n",
    "    \"A photo of a bicycle\", \n",
    "    \"A photo of a plane\"\n",
    "]\n",
    "inputs = processor(text=texts, images=[image], return_tensors=\"pt\", padding=True).to(device)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "logits_per_text = outputs.logits_per_text\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "print(\"Probabilities for multiple texts:\", probs)\n",
    "\n",
    "# Plot the probabilities\n",
    "probs = probs.cpu().detach().numpy().flatten()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(texts, probs, color='skyblue')\n",
    "plt.xlabel('Probability')\n",
    "plt.title('Model Confidence for Different Text Prompts')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17022db5-0a1c-4f8f-9a2f-f4ec1bf440c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
